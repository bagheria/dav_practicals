---
title: "Unsupervised learning: Clustering"
params:
  answers: true
mainfont: Arial
fontsize: 12pt
urlcolor: blue
output: 
  html_document:
    toc: true
    toc_depth: 1
    toc_float: true
    df_print: paged
    theme: paper
    # pandoc_args: --output=unsupervised_learning_2_answers.html
  pdf_document:
    toc: true
    toc_depth: 1
    latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
In this practical, we will learn how to perform clustering.

We will use the packages `igraph`, `ggdendro`, and `dendextend`. For this, you will probably need to `install.packages()` before running the `library()` functions.

```{r packages, warning = FALSE, message = FALSE}
library(igraph)
library(ggdendro)
library(dendextend)
library(ISLR)
library(tidyverse)
```


```{r seed, include = FALSE}
set.seed(123)
```

---

1. __Load the dataset `data/clusterdata.csv`. Create a scatter plot of this dataset, mapping `x1` to the x position and `x2` to the y position. Use `coord_fixed()` to ensure that the x and y axes are the same size w.r.t. their values.__

---

```{r clusdat, message = FALSE, include = params$answers}

clus_df <- read_csv("data/clusterdata.csv")

clus_df %>% 
  ggplot(aes(x = x1, y = x2)) +
  geom_point() +
  coord_fixed() +
  theme_minimal()

```

# K-means clustering

The `kmeans()` function implements k-means clustering.

---

2. __Create two cluster objects with the `kmeans()` function using the same data, one with 3 clusters and one with 5 clusters.__

---

```{r kmeans1, include = params$answers}

kmeans_3 <- kmeans(clus_df, centers = 3)
kmeans_5 <- kmeans(clus_df, centers = 5)

```

---

3. __Create two scatterplots where you map the cluster assignment of the cluster objects to the colour of the points.__

---

```{r plots, include = params$answers}
# We can use the cowplot package to display these next to one another

cowplot::plot_grid(
  
  # K = 3 plot
  clus_df %>% 
    ggplot(aes(x = x1, y = x2, colour = factor(kmeans_3$cluster))) +
    geom_point() +
    coord_fixed() +
    theme_minimal() +
    scale_colour_viridis_d(guide = "none"),
  
  # K = 5 plot
  clus_df %>% 
    ggplot(aes(x = x1, y = x2, colour = factor(kmeans_5$cluster))) +
    geom_point() +
    coord_fixed() +
    theme_minimal() +
    scale_colour_viridis_d(guide = "none")
  
)

```

# Hierarchical clustering

The `hclust()` function implements hierarchical clustering. 

---

4. __Compute hierarchical cluster objects with the `hclust()` function using the same data, one with complete-linkage and one with average-linkage. (Hint: use `dist()` function to produce dissimilarity structure)__

---

```{r hclust, include = params$answers}

# Compute distance matrix
dist_matrix <- dist(clus_df, method = "euclidean")
# Compute 2 hierarchical clusterings
hclust_mod1 <- hclust(dist_matrix, method = "complete")
hclust_mod2 <- hclust(dist_matrix, method = "average")

```

---

5. __Use the `ggdendrogram()` function from `library(ggdendro)` to plot two dendrograms for the clustering objects.__

---

```{r ggdendro, include = params$answers, fig.width=12}

ggdendrogram(hclust_mod1) + labs(title="Complete-linkage Hierarchical clustering")
ggdendrogram(hclust_mod2) + labs(title="Average-linkage Hierarchical clustering")

```

---

6. __Now we want to compare dendrograms. First start by transforming the results as dendrograms and create a list to hold the two dendrograms using `dendlist()` function. And then visualise the comparison of two dendrograms with `tanglegram()` function.__

---

```{r dendrogramscomparison, include = params$answers, fig.retina=TRUE, fig.height=12, fig.width=10}

# Create two dendrograms
dend1 <- as.dendrogram (hclust_mod1)
dend2 <- as.dendrogram (hclust_mod2)

# Create a list to hold dendrograms
dend_list <- dendlist(dend1, dend2)

# Align and plot two dendrograms side by side
dend_list %>%
  untangle(method = "step1side") %>% # Find the best alignment layout
  tanglegram()                       # Draw the two dendrograms

```


---

7. __Does complete-linkage hierarchical clustering with a cutoff at 3 clusters yield the same result as 3-means clustering? Hint: use the `cutree()` function to cut off the hierarchical clustering object at 3 clusters.__

---

```{r hclustkmeans, include = params$answers}
# first, let's make two factors with congruent labels for the observations
hclust_fac <- factor(cutree(hclust_mod1, k = 3), labels = c("a", "b", "c"))
kmeans_fac <- factor(kmeans_3$cluster, labels = c("b", "c", "a"))

# then we can check whether the labels are the same
same_clust <- hclust_fac == kmeans_fac
all(same_clust)
# So there are differences. How many?
sum(!same_clust)

# we could also make a visual comparison of the differences
ggplot(clus_df, aes(x = x1, y = x2)) +
  geom_point(size = 6, colour = ifelse(same_clust, "#00000000", "#00000030")) +
  geom_point(aes(colour = hclust_fac), position = position_nudge(-0.07)) +
  geom_point(aes(colour = kmeans_fac), position = position_nudge(0.07)) +
  scale_colour_viridis_d() +
  coord_fixed() +
  theme_minimal() +
  theme(legend.position = "none")

```


# Programming assignment: manual K-means clustering

The euclidian distance between two vectors $\mathbf{x}$  and $\mathbf{y}$ of length $n$ is $D = || \mathbf{x} - \mathbf{y} ||_2 = \sqrt{\sum_{i = 1}^n (x_i - y_i)^2}$. These two vectors represent points in $n$-dimensional space and the euclidian distance is the straight-line distance between these points.

---

8. __Write a function `l2_dist(x, y)` that takes in two vectors and outputs the euclidian distance between the two vectors.__

---

```{r distfun, , include = params$answers}
# there are several ways to do this. Here are two.
# way 1 (slow):
l2_dist <- function(x, y) {
  sumsq <- 0
  for (i in 1:length(x)) {
    sumsq <- sumsq + (x[i] - y[i])^2
  }
  return(sqrt(sumsq))
}

# way 2 (fast):
l2_dist <- function(x, y) {
  sqrt(sum((x - y)^2))
}
```


---

9. __Program a k-means clustering algorithm and apply it to this data. Use Algorithm 10.1 from the ISLR book. Visualise your result.__

---

```{r clusalg, , include = params$answers}
# as with any program, there are many ways to do this! The example answer is
# not necessarily the best answer.

my_clus <- function(X, k) {
  # 1. Randomly assign a number, from 1 to K, to each of the observations.
  # These serve as initial cluster assignments for the observations.
  n <- nrow(X)
  p <- ncol(X)
  clus_vec <- sample(rep(1:k, length.out = n))
  
  # 2. Iterate until the cluster assignments stop changing:
  changed <- TRUE
  while (changed) {
    
    # (a) For each of the K clusters, compute the cluster centroid
    centroids <- matrix(NA, nrow = k, ncol = p)
    for (i in 1:k) {
      X_clus <- X[clus_vec == i, ]
      centroids[i, ] <- colMeans(X_clus)
    }
    
    # (b) Assign each observation to the cluster whose centroid is closest
    clus_vec_old <- clus_vec
    for (i in 1:n) {
      
      distances <- rep(0, k)
      for (j in 1:k) {
        distances[j] <- l2_dist(X[i, ], centroids[j, ])
      }
      
      clus_vec[i] <- which.min(distances)
    }
    
    changed <- !identical(clus_vec_old, clus_vec)
  }
  
  # Return the centroids and the cluster assignments
  list(
    assign    = clus_vec, 
    centroids = centroids
  )
}


# let's use it
clus_result <- my_clus(clus_df, 3)

# now to plot and compare
clus_df %>% 
  ggplot(aes(x = x1, y = x2, colour = factor(clus_result$assign))) +
  geom_point() +
  coord_fixed() +
  theme_minimal() +
  scale_colour_viridis_d(guide = "none")
```